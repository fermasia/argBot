{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones para encontrar usuarios a testear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from IPython.display import HTML, display\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import Comment\n",
    "import json\n",
    "import html\n",
    "import re\n",
    "import urllib.parse\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuro proxy y headers\n",
    "url_proxy = 'https://free-proxy-list.net/anonymous-proxy.html'\n",
    "response_pxy = requests.get(url_proxy)\n",
    "pxySoup = BeautifulSoup(response_pxy.text, 'html.parser')\n",
    "rowsPxy = []\n",
    "headPxy = []\n",
    "for tablePart in pxySoup.find_all(attrs={'id':'proxylisttable'})[0].find_all(True, recursive=False):\n",
    "    if(tablePart.name == 'thead'):\n",
    "        for row in tablePart.find_all(True, recursive=False):\n",
    "            headPxy = [tr.text for tr in row]\n",
    "    if(tablePart.name == 'tbody'):\n",
    "        for row in tablePart.find_all(True, recursive=False):\n",
    "            rowsPxy.append([tr.text for tr in row])\n",
    "proxiesDf = pd.DataFrame(data=rowsPxy, columns=headPxy)\n",
    "proxiesDf = proxiesDf[(proxiesDf['Anonymity']=='elite proxy')].sort_values(by='Https', ascending=False)\n",
    "headersChrome = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36'\n",
    "}\n",
    "proxies = {\n",
    "    \"http\": proxiesDf[proxiesDf.Https=='no'].loc[:,'IP Address'].values[0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['min_position', 'has_more_items', 'items_html', 'new_latent_count', 'focused_refresh_interval'])\n"
     ]
    }
   ],
   "source": [
    "headersMore = {\n",
    "    'Accept': 'application/json, text/javascript, */*; q=0.01',\n",
    "    'Host': 'twitter.com',\n",
    "    'Accept-Language': 'es-AR,es;q=0.8,en-US;q=0.5,en;q=0.3',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'X-Twitter-Active-User': 'yes',\n",
    "    'X-Requested-With': 'XMLHttpRequest',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36'\n",
    "}\n",
    "\n",
    "def getNewPage(term, searchType, maxPosition):\n",
    "    pageResponse = requests.get(\n",
    "        'https://twitter.com/i/search/timeline',\n",
    "        params={\n",
    "            'vertical': 'news',\n",
    "            'q':'#'+term,\n",
    "            'max_position': maxPosition,\n",
    "            'include_entities': 1,\n",
    "            'include_available_features': 1,\n",
    "            'vertical': 'news',\n",
    "            'reset_error_state': False\n",
    "        },\n",
    "        proxies=proxies,\n",
    "        headers=headersMore\n",
    "    )\n",
    "    print(pageResponse.json().keys())\n",
    "\n",
    "def searchTerm(term, searchType='q'):\n",
    "    pageResponse = False\n",
    "    if(searchType == 'q'):\n",
    "        pageResponse = requests.get('https://twitter.com/search?q='+term, headers=headersChrome, proxies=proxies)\n",
    "    else:\n",
    "        pageResponse = requests.get('https://twitter.com/hashtag/'+term, headers=headersChrome, proxies=proxies)\n",
    "    twData = BeautifulSoup(pageResponse.text, 'html.parser')\n",
    "    initialStream = twData.findAll(attrs={'id':'timeline'})[0].findAll(attrs={'class':'stream-container'})[0]\n",
    "    dataMax = initialStream.attrs['data-max-position']\n",
    "    dataMin = initialStream.attrs['data-min-position']\n",
    "    streamItems = initialStream.findAll(attrs={'id':'stream-items-id'})[0]\n",
    "    tweetList = streamItems.findAll('li', recursive=False)\n",
    "#     for tweet in tqdm(tweetList):\n",
    "#         print(\n",
    "#             tweet.findAll(attrs={'class':'username'})[0].text[1:]\n",
    "#         )\n",
    "    maxPages = 1\n",
    "    for pageNum in range(maxPages):\n",
    "        response = getNewPage(term, searchType, dataMax)\n",
    "searchTerm('cfk', '#')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
